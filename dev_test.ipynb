{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cleaning/workspace/KoSimCSE/.venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from data.info import (\n",
    "    DataName,\n",
    "    DataPath,\n",
    "    STSDatasetFeatures,\n",
    "    TCDatasetFeatures,\n",
    "    TrainType,\n",
    "    FileFormat,\n",
    "    TrainType,\n",
    "    UnsupervisedSimCseFeatures,\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_from_disk('data/datasets/train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    dev: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1466\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1379\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1466\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1379\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.7.13 ('.venv': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/home/cleaning/workspace/KoSimCSE/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO\n",
    ")\n",
    "\n",
    "\n",
    "wiki_dataset = load_dataset(\"sh110495/kor-wikipedia\")\n",
    "#######################################################\n",
    "data = []\n",
    "for wiki_text in tqdm(wiki_dataset[\"train\"][\"text\"]):\n",
    "    wiki_text = wiki_text.replace(\". \", \".\\n\")\n",
    "    wiki_text = wiki_text.replace(\"\\xa0\", \" \")\n",
    "    wiki_sentences = wiki_text.split(\"\\n\")\n",
    "\n",
    "    for wiki_sentence in wiki_sentences:\n",
    "        wiki_sentence = wiki_sentence.rstrip().lstrip()\n",
    "        if len(wiki_sentence) >= 10:\n",
    "            data.append(wiki_sentence)\n",
    "# data/utils.py로 옮기기\n",
    "###########################################################\n",
    "wiki_df = pd.DataFrame(data={UnsupervisedSimCseFeatures.SENTENCE.value: data})\n",
    "wiki_df[UnsupervisedSimCseFeatures.SENTENCE.value] = wiki_df[\n",
    "    UnsupervisedSimCseFeatures.SENTENCE.value\n",
    "].apply(wiki_preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "sts_preprocess\n",
    "\"\"\"\n",
    "\n",
    "train_floder_path = get_folder_path(root=DataPath.ROOT, sub=DataPath.TRAIN)\n",
    "dev_floder_path = get_folder_path(root=DataPath.ROOT, sub=DataPath.DEV)\n",
    "test_floder_path = get_folder_path(root=DataPath.ROOT, sub=DataPath.TEST)\n",
    "\n",
    "preprocess_wiki_data_path = get_data_path(\n",
    "    folder_path=train_floder_path,\n",
    "    data_source=DataName.PREPROCESS_WIKI,\n",
    "    train_type=TrainType.TRAIN,\n",
    "    file_format=FileFormat.CSV,\n",
    ")\n",
    "wiki_df = wiki_df.dropna(axis=0)\n",
    "wiki_df.to_csv(preprocess_wiki_data_path, index=False)\n",
    "# wiki_df = pd.read_csv(preprocess_wiki_data_path)\n",
    "logging.info(\n",
    "    f\"preprocess wiki train done!\\nfeatures:{wiki_df.columns} \\nlen: {len(wiki_df)}\\nna count:{sum(wiki_df[UnsupervisedSimCseFeatures.SENTENCE.value].isna())}\"\n",
    ")\n",
    "\n",
    "klue_train = raw_data_to_dataframe(\n",
    "    DataPath.ROOT, DataPath.RAW, DataName.RAW_KLUE, TrainType.TRAIN, FileFormat.JSON\n",
    ")\n",
    "\n",
    "kakao_train = raw_data_to_dataframe(\n",
    "    DataPath.ROOT, DataPath.RAW, DataName.RAW_KAKAO, TrainType.TRAIN, FileFormat.TSV\n",
    ")\n",
    "\n",
    "kakao_dev = raw_data_to_dataframe(\n",
    "    DataPath.ROOT, DataPath.RAW, DataName.RAW_KAKAO, TrainType.DEV, FileFormat.TSV\n",
    ")\n",
    "\n",
    "kakao_test = raw_data_to_dataframe(\n",
    "    DataPath.ROOT, DataPath.RAW, DataName.RAW_KAKAO, TrainType.TEST, FileFormat.TSV\n",
    ")\n",
    "####\n",
    "# tc\n",
    "tc_train = raw_data_to_dataframe(\n",
    "    DataPath.ROOT, DataPath.RAW, DataName.RAW_TC, TrainType.TRAIN, FileFormat.JSON\n",
    ")\n",
    "tc_dev = raw_data_to_dataframe(\n",
    "    DataPath.ROOT, DataPath.RAW, DataName.RAW_TC, TrainType.DEV, FileFormat.JSON\n",
    ")\n",
    "\n",
    "tc_train = change_col_name(\n",
    "    tc_train, TCDatasetFeatures.TITLE, UnsupervisedSimCseFeatures.SENTENCE\n",
    ")\n",
    "tc_dev = change_col_name(\n",
    "    tc_dev, TCDatasetFeatures.TITLE, UnsupervisedSimCseFeatures.SENTENCE\n",
    ")\n",
    "print(tc_dev.head())\n",
    "print(wiki_df.head())\n",
    "#\n",
    "####\n",
    "unsup_datas = [wiki_df, tc_train, tc_dev]\n",
    "total_sen = []\n",
    "for unsup_data in unsup_datas:\n",
    "    total_sen.extend(unsup_data[UnsupervisedSimCseFeatures.SENTENCE].to_list())\n",
    "total_unsup_df = pd.DataFrame(data={UnsupervisedSimCseFeatures.SENTENCE: total_sen})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/18/2022 22:51:28 - WARNING - datasets.builder -   Using custom data configuration default-3b562c3513b8af77\n",
      "04/18/2022 22:51:28 - WARNING - datasets.builder -   Reusing dataset csv (./data/.cache/csv/default-3b562c3513b8af77/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a6aaeb677f425783b7ffb520fe89dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_data_files = {}\n",
    "eval_data_files['dev'] = 'data/dev/sts_dev.tsv'\n",
    "valid_dataset = load_dataset(\n",
    "        \"csv\",\n",
    "        data_files=eval_data_files,\n",
    "        cache_dir=\"./data/.cache\",\n",
    "        delimiter=\"\\t\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(valid_dataset['dev'])):\n",
    "    if not isinstance(valid_dataset['dev']['score'][i],float):\n",
    "        print('dd',valid_dataset['dev']['score'][i],type(valid_dataset['dev']['score'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr = load_metric(\"pearsonr\").compute\n",
    "t = pearsonr(predictions=[1,1,2], references=[2,2,1])\n",
    "t['pearsonr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from data.info import (\n",
    "    DataName,\n",
    "    DataPath,\n",
    "    STSDatasetFeatures,\n",
    "    TCDatasetFeatures,\n",
    "    TrainType,\n",
    "    FileFormat,\n",
    "    TrainType,\n",
    "    UnsupervisedSimCseFeatures,\n",
    ")\n",
    "from data.utils import (\n",
    "    get_data_path,\n",
    "    get_folder_path,\n",
    "    raw_data_to_dataframe,\n",
    "    make_unsupervised_sentence_data,\n",
    "    wiki_preprocess,\n",
    "    change_col_name,\n",
    "    add_sts_df,\n",
    ")\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "wiki_preprocess\n",
    "\"\"\"\n",
    "wiki_dataset = load_dataset(\"sh110495/kor-wikipedia\")\n",
    "#######################################################\n",
    "data = []\n",
    "for wiki_text in tqdm(wiki_dataset[\"train\"][\"text\"]):\n",
    "    wiki_text = wiki_text.replace(\". \", \".\\n\")\n",
    "    wiki_text = wiki_text.replace(\"\\xa0\", \" \")\n",
    "    wiki_sentences = wiki_text.split(\"\\n\")\n",
    "\n",
    "    for wiki_sentence in wiki_sentences:\n",
    "        wiki_sentence = wiki_sentence.rstrip().lstrip()\n",
    "        if len(wiki_sentence) >= 10:\n",
    "            data.append(wiki_sentence)\n",
    "# data/utils.py로 옮기기\n",
    "###########################################################\n",
    "wiki_df = pd.DataFrame(data={UnsupervisedSimCseFeatures.SENTENCE.value: data})\n",
    "wiki_df[UnsupervisedSimCseFeatures.SENTENCE.value] = wiki_df[\n",
    "    UnsupervisedSimCseFeatures.SENTENCE.value\n",
    "].apply(wiki_preprocess)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "sts_preprocess\n",
    "\"\"\"\n",
    "\n",
    "train_floder_path = get_folder_path(root=DataPath.ROOT, sub=DataPath.TRAIN)\n",
    "dev_floder_path = get_folder_path(root=DataPath.ROOT, sub=DataPath.DEV)\n",
    "test_floder_path = get_folder_path(root=DataPath.ROOT, sub=DataPath.TEST)\n",
    "\n",
    "preprocess_wiki_data_path = get_data_path(\n",
    "    folder_path=train_floder_path,\n",
    "    data_source=DataName.PREPROCESS_WIKI,\n",
    "    train_type=TrainType.TRAIN,\n",
    "    file_format=FileFormat.CSV,\n",
    ")\n",
    "wiki_df = wiki_df.dropna(axis=0)\n",
    "wiki_df.to_csv(preprocess_wiki_data_path, index=False)\n",
    "# wiki_df = pd.read_csv(preprocess_wiki_data_path)\n",
    "logging.info(\n",
    "    f\"preprocess wiki train done!\\nfeatures:{wiki_df.columns} \\nlen: {len(wiki_df)}\\nna count:{sum(wiki_df[UnsupervisedSimCseFeatures.SENTENCE.value].isna())}\"\n",
    ")\n",
    "\n",
    "klue_train = raw_data_to_dataframe(\n",
    "    DataPath.ROOT, DataPath.RAW, DataName.RAW_KLUE, TrainType.TRAIN, FileFormat.JSON\n",
    ")\n",
    "\n",
    "kakao_train = raw_data_to_dataframe(\n",
    "    DataPath.ROOT, DataPath.RAW, DataName.RAW_KAKAO, TrainType.TRAIN, FileFormat.TSV\n",
    ")\n",
    "\n",
    "kakao_dev = raw_data_to_dataframe(\n",
    "    DataPath.ROOT, DataPath.RAW, DataName.RAW_KAKAO, TrainType.DEV, FileFormat.TSV\n",
    ")\n",
    "\n",
    "kakao_test = raw_data_to_dataframe(\n",
    "    DataPath.ROOT, DataPath.RAW, DataName.RAW_KAKAO, TrainType.TEST, FileFormat.TSV\n",
    ")\n",
    "####\n",
    "# tc\n",
    "tc_train = raw_data_to_dataframe(\n",
    "    DataPath.ROOT, DataPath.RAW, DataName.RAW_TC, TrainType.TRAIN, FileFormat.JSON\n",
    ")\n",
    "tc_dev = raw_data_to_dataframe(\n",
    "    DataPath.ROOT, DataPath.RAW, DataName.RAW_TC, TrainType.DEV, FileFormat.JSON\n",
    ")\n",
    "\n",
    "tc_train = change_col_name(\n",
    "    tc_train, TCDatasetFeatures.TITLE, UnsupervisedSimCseFeatures.SENTENCE\n",
    ")\n",
    "tc_dev = change_col_name(\n",
    "    tc_dev, TCDatasetFeatures.TITLE, UnsupervisedSimCseFeatures.SENTENCE\n",
    ")\n",
    "print(tc_dev.head())\n",
    "print(wiki_df.head())\n",
    "#\n",
    "####\n",
    "unsup_datas = [wiki_df, tc_train, tc_dev]\n",
    "total_sen = []\n",
    "for unsup_data in unsup_datas:\n",
    "    total_sen.extend(unsup_data[UnsupervisedSimCseFeatures.SENTENCE].to_list())\n",
    "total_unsup_df = pd.DataFrame(data={UnsupervisedSimCseFeatures.SENTENCE: total_sen})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, OurTrainingArguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if is_main_process(4) else logging.WARN,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2022 23:47:40 - INFO - __main__ -   Training/evaluation parameters %s\n"
     ]
    }
   ],
   "source": [
    "if is_main_process(4):\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "logger.info(\"Training/evaluation parameters %s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/16/2022 23:54:12 - WARNING - datasets.builder -   Using custom data configuration default-5b3a05775df14031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to ./data/csv/default-5b3a05775df14031/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdaa676507b41c1a4c90b58478f2f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d142d0bb9159401884343350c84c9f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to ./data/csv/default-5b3a05775df14031/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be6d5d8ea3a4c079d32d4105f885647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extension = \"csv\"\n",
    "datasets = load_dataset(extension, data_files=\"data/train/wiki_train.csv\", cache_dir=\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForCL: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForCL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForCL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForCL were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['mlp.dense.weight', 'mlp.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForCL.from_pretrained(\"klue/bert-base\",model_args=ModelArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32000, 768, padding_idx=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent0_cname = column_names[0]\n",
    "sent1_cname = column_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentence'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent0_cname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence'],\n",
       "        num_rows: 2861157\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2_cname = None\n",
    "def prepare_features(examples):\n",
    "    # padding = longest (default)\n",
    "    #   If no sentence in the batch exceed the max length, then use\n",
    "    #   the max sentence length in the batch, otherwise use the \n",
    "    #   max sentence length in the argument and truncate those that\n",
    "    #   exceed the max length.\n",
    "    # padding = max_length (when pad_to_max_length, for pressure test)\n",
    "    #   All sentences are padded/truncated to data_args.max_seq_length.\n",
    "    total = len(examples[sent0_cname])\n",
    "\n",
    "    # Avoid \"None\" fields \n",
    "    for idx in range(total):\n",
    "        if examples[sent0_cname][idx] is None:\n",
    "            examples[sent0_cname][idx] = \" \"\n",
    "        if examples[sent1_cname][idx] is None:\n",
    "            examples[sent1_cname][idx] = \" \"\n",
    "    \n",
    "    sentences = examples[sent0_cname] + examples[sent1_cname]\n",
    "\n",
    "    sent_features = tokenizer(\n",
    "        sentences,\n",
    "        max_length=32,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    features = {}\n",
    "    if sent2_cname is not None:\n",
    "        for key in sent_features:\n",
    "            features[key] = [[sent_features[key][i], sent_features[key][i+total], sent_features[key][i+total*2]] for i in range(total)]\n",
    "    else:\n",
    "        for key in sent_features:\n",
    "            features[key] = [[sent_features[key][i], sent_features[key][i+total]] for i in range(total)]\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61bf47bdd0d4d529e6093c651b4153b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/716 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355871d653494aecbf267cf64e0de16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/716 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c0fa42b981450da6c538cc9510f273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/716 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af74930180d4444a1a72cd1c7764923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/716 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = datasets[\"train\"].map(prepare_features,batched=True,num_proc=4,remove_columns=column_names,load_from_cache_file=not DataTrainingArguments.overwrite_cache,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 2861157\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81032ca24206c3b5ec8daf7f1877a1f9a3df4bda8ff335de3fc33f9a4523828c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
